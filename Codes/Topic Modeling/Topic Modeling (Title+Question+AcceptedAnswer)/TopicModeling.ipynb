{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import unicodedata\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "import json\n",
    "import html\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "pyLDAvis.enable_notebook()\n",
    "os.environ.update({'MALLET_HOME':'./Mallet'}) \n",
    "mallet_path = './Mallet/bin/mallet' \n",
    "current_dir = os.getcwd()\n",
    "coherence_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(o):\n",
    "    if isinstance(o, np.int64): return int(o)  \n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words set\n",
    "STOP_WORDS_FILES = ['mallet_stop_words.txt', 'custom_stop_words.txt']\n",
    "stop_words_set = set()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for word in stop_words:\n",
    "    if('\\'' in word):\n",
    "        stop_words_set.add(word.strip().replace('\\'', ''))\n",
    "    stop_words_set.add(word)\n",
    "for swfile in STOP_WORDS_FILES:\n",
    "    try:\n",
    "        with open(swfile, 'r') as f:\n",
    "            words = f.readlines()\n",
    "            for word in words:\n",
    "                if('\\'' in word):\n",
    "                    stop_words_set.add(word.strip().replace('\\'', ''))\n",
    "                stop_words_set.add(word.strip())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove non ascii\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\n",
    "        'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = text.lower()\n",
    "    # unescaping\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r'<code>(.|\\n)*?</code>','', text) # removing <code>...</code>\n",
    "    text = re.sub(r'<a.*?</a>', '', text)  # removing whole anchor tags\n",
    "    text = re.sub(r'(<.*?>)', '', text)  # removing html markup\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # removing punctuation\n",
    "    text = re.sub(r'[\\d]', '', text)  # removing digits\n",
    "    # remove stopwords\n",
    "    tokenized = []\n",
    "    for word in text.split():\n",
    "        if word in stop_words_set:\n",
    "            continue\n",
    "        tokenized.append(word)\n",
    "    for i in range(len(tokenized)):\n",
    "        word = tokenized[i]\n",
    "        word = WordNetLemmatizer().lemmatize(word, pos='v')\n",
    "        tokenized[i] = stemmer.stem(word)\n",
    "        # tokenized[i] = word\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"so_body.csv\")\n",
    "df['preprocessed'] = \"\"\n",
    "for i in range(len(df)):\n",
    "    df['preprocessed'].iloc[i] = preprocess_text(df.raw.iloc[i])\n",
    "df.to_csv('preprocesseedData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(parent_dir, dir_name):\n",
    "    temp = os.path.join(parent_dir,dir_name)\n",
    "    try:  \n",
    "        os.mkdir(temp)  \n",
    "    except OSError as error:  \n",
    "        # print(error)\n",
    "        pass\n",
    "    return temp\n",
    "\n",
    "def make_link(id,type):\n",
    "    '''\n",
    "    id = postid\n",
    "    type : 'q' for question\n",
    "           'a' for answer\n",
    "    '''\n",
    "    url = f'https://stackoverflow.com/{type}/{id}'\n",
    "    return f'=HYPERLINK(\"{url}\", \"{id}\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out optimum topic number\n",
    "data = df['preprocessed']\n",
    "dictionary = gensim.corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "coherence_scores = []\n",
    "for num_topics in tqdm(range(5,31)):\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=50/num_topics)\n",
    "    coherenceModel = CoherenceModel(model=ldamallet, texts=data, dictionary=dictionary, coherence='c_v')\n",
    "    score = coherenceModel.get_coherence()\n",
    "    coherence_scores.append([num_topics,score])\n",
    "# save scores as csv\n",
    "ch_df = pd.DataFrame(coherence_scores,columns=['Num Topic','Score'])\n",
    "ch_df.to_csv('Coherence_Scores.csv')\n",
    "# plot\n",
    "plt.xlabel('Number of Topics') \n",
    "plt.ylabel('Coherence Score') \n",
    "x = []\n",
    "y = []\n",
    "for score in coherence_scores:\n",
    "    x.append(score[0])\n",
    "    y.append(score[1])\n",
    "plt.plot(x,y,c='r')\n",
    "plt.gca().set_aspect('auto')\n",
    "plt.grid()\n",
    "plt.savefig('scores.png', dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = create_dir(current_dir, 'Topic Modeling Results')\n",
    "data = df['preprocessed']\n",
    "dictionary = gensim.corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "# create folder for topic number\n",
    "NUM_TOPIS = [5,9,6,11,10,8,7,17,14]\n",
    "for num_topics in NUM_TOPIS:\n",
    "    topic_dir = create_dir(res_dir, f'{num_topics} Topics')\n",
    "    if os.path.isfile(os.path.join(topic_dir, 'ldamallet.pkl')):\n",
    "        ldamallet = pickle.load(\n",
    "            open(os.path.join(topic_dir, 'ldamallet.pkl'), \"rb\"))\n",
    "    else:\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "            mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha=50/num_topics)\n",
    "        # save the model as pickle\n",
    "        pickle.dump(ldamallet, open(os.path.join(\n",
    "            topic_dir, 'ldamallet.pkl'), \"wb\"))\n",
    "    topics = [[(word, word_prob) for word, word_prob in ldamallet.show_topic(\n",
    "        n, topn=30)] for n in range(ldamallet.num_topics)]\n",
    "    # term-topic matrix\n",
    "    topics_df = pd.DataFrame([[f'{word} {round(word_prob,4)}' for word, word_prob in topic] for topic in topics], columns=[\n",
    "        f'Term {i}' for i in range(1, 31)], index=[f'Topic {n}' for n in range(ldamallet.num_topics)]).T\n",
    "    topics_df.to_csv(os.path.join(topic_dir, 'term x topic.csv'))\n",
    "    # topic words\n",
    "    topic_words_dir = create_dir(topic_dir, 'TopicWords')\n",
    "    for n in range(num_topics):\n",
    "        topic_words_df = pd.DataFrame(\n",
    "            [[word_prob, word]for word, word_prob in topics[n]], columns=['Prob', 'Word'])\n",
    "        topic_words_df.to_csv(os.path.join(topic_words_dir, f'{n}.csv'))\n",
    "    # post to dominant topic\n",
    "    corpus_topic_df = pd.DataFrame()\n",
    "    corpus_topic_df['link'] = df.id\n",
    "    corpus_topic_df['qa'] = df.qa\n",
    "    for i in range(len(corpus_topic_df)):\n",
    "        corpus_topic_df.link.iloc[i] = make_link(df.id.iloc[i],df.qa.iloc[i])\n",
    "    topic_model_results = ldamallet[corpus]\n",
    "    corpus_topics = [sorted(doc, key=lambda x: -x[1])[0]\n",
    "                    for doc in topic_model_results]\n",
    "    corpus_topic_df['Dominant Topic'] = [item[0] for item in corpus_topics]\n",
    "    corpus_topic_df['Correlation'] = [item[1] for item in corpus_topics]\n",
    "    corpus_topic_df.to_csv(os.path.join(topic_dir, 'postToTopic.csv'))\n",
    "    topic_to_post_dir = create_dir(topic_dir, 'TopicToPost')\n",
    "    for i in range(num_topics):\n",
    "        temp = create_dir(topic_to_post_dir, str(i))\n",
    "        temp_q_df = corpus_topic_df.loc[corpus_topic_df['Dominant Topic'] == i]\n",
    "        temp_q_df = temp_q_df.loc[temp_q_df['qa'] == 'q']\n",
    "        temp_a_df = corpus_topic_df.loc[corpus_topic_df['Dominant Topic'] == i]\n",
    "        temp_a_df = temp_a_df.loc[temp_a_df['qa'] == 'a']\n",
    "        temp_q_df.drop(columns=['Dominant Topic','qa']).to_csv(\n",
    "            os.path.join(temp, 'Questions.csv'), index=False)\n",
    "        temp_q_df.drop(columns=['Dominant Topic','qa']).to_excel(\n",
    "            os.path.join(temp, 'Questions.xlsx'), index=False)\n",
    "        temp_a_df.drop(columns=['Dominant Topic','qa']).to_csv(\n",
    "            os.path.join(temp, 'Answers.csv'), index=False)\n",
    "        temp_a_df.drop(columns=['Dominant Topic','qa']).to_excel(\n",
    "            os.path.join(temp, 'Answers.xlsx'), index=False)\n",
    "    # post count under any topic\n",
    "    topic_post_cnt_df = corpus_topic_df.groupby('Dominant Topic').agg(\n",
    "        Document_Count=('Dominant Topic', np.size),\n",
    "        Percentage=('Dominant Topic', np.size)).reset_index()\n",
    "    topic_post_cnt_df['Percentage'] = topic_post_cnt_df['Percentage'].apply(\n",
    "        lambda x: round((x*100) / len(corpus), 2))\n",
    "    topic_post_cnt_df.to_csv(os.path.join(topic_dir, 'postPerTopic.csv'))\n",
    "    # pyLDAvis\n",
    "    vis = pyLDAvis.gensim.prepare(\n",
    "        gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet), corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, os.path.join(topic_dir, f'pyLDAvis.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venvtp': venv)",
   "language": "python",
   "name": "python38564bitvenvtpvenv33480300a6c04ad2acb8ed446415cfe0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}