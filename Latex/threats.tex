\section{Threats to Validity}\label{subsec:validity}
% \bf{Internal validity.} This type of threat refers to the mistakes or errors in our
% experiments such as missing some related questions regarding low-code
% development if the questions have not been tagged properly. In order to mitigate this, we examine all the tags that we find in the low-code related questions. Then we use $S_{tag}$ and ${R_tag}$ values to find other tags that are significantly related to our existing low-code tags. This approach and our $S_{tag}$ and ${R_tag}$ values are in line with other related works\cite{chatbot}\cite{ahmed2018concurrency}\cite{rosen2016mobile}\cite{bagherzadeh2019going} that used this measure to find better coverage of a certain SE topic.

% Another potential threat is regarding topic modeling technique where we choice of $K$ = 13 as the optimal number of topics for our dataset $B$. This optimal number of topics have direct impact on the output of LDA. We experimented with different values of $K$ following related works\cite{chatbot}\cite{bagherzadeh2019going}. We used coherence score and manual examination to find the optimal value for $K$ that gives us most relevant and generalized low-code related topics.

% Our manual labelling of questions to agile SDLC phase is another threat to validity. In order to alleviate this each question was labelled by two authors and a third author weighted in in case there were a conflict.

% checked to
% find related tags. There might be some making coding error during LDA topic
% modelling and SO post extractions. This data extraction have have reviewed by
% two authors. Our studied X SO posts regarding LCDP. We selected these posts by
% choosing questions that has at least one tags of some popular LCDP. We have not
% takes posts that does not have LCDP tags. we reported our LDA parameters for
% replications Tags were selected in an open debate session
% difficult to find optimal value of K. We used different values of K and calculated topic coherence score. As LDA is probabilic, we ran our model couple of times and find not significant difference.
% We tried several well known techniques to extract related tags \cite{rosen2016mobile}\cite{bagherzadeh2019going}.
% We tried several well known techniques to manual labelling \cite{bajaj2014mining}\cite{ahmed2018concurrency}.
% We tried several well known techniques to find k \cite{ahmed2018concurrency}\cite{barua2014developers}.
% Only SO might not show the whole picture because due the platform's question's policies, there are some non technical but important challenges such as general soft ware architecture/design questions, comparison between platforms, platform's reputation, support and scalability,
% However large number of  SO questions and community members comments help to mitigate this.

% \bf{External Validity.} This type of threats concern about the generalization of the findings of our study. Our study is based on data from developers discussion on Stack Overflow. However, there are other forums that developer may use to discussion on low-code platform related challenges. But we believe using the data from Stack Overflow provides us the generalizability because Stack Overflow is widely used Q\&A platform for developers from different background. In order to ensure good quality discussion we only used questions with no-negative scores and accepted answers only. However we also believe this study can be improved by including low-code developers' discussions from other forums as well as surveying and interviewing low-code developers about their challenges.

\bf{Internal validity} threats relate to the authors'
bias while conducting the analysis. We mitigate the bias in
our manual labeling of topics and  LCSD phases by consulting the labels among multiple authors. Four of the authors
actively participated in the labelling process. The third author reviewed the final labels and refined the labels by consulting with the first author. \bf{Construct Validity} threats 
relate to the errors that may occur in data collection like, identifying relevant LCSD tags. To mitigate this, we examine all the tags that we find in the low-code related questions. Then we expanded our tag list using state-of-art approach~\cite{bagherzadeh2019going,abdellatif2020challenges,ahmed2018concurrency, rosen2016mobile}. Another potential threat is the topic modeling technique, where we choose $K$ = 13 as the optimal number of topics for our dataset $B$. This optimal number of topics have a direct impact on the output of LDA. We experimented with different values of $K$ following related works~\cite{abdellatif2020challenges, bagherzadeh2019going}. We used the coherence score and manual examination to find $K$'s optimal that gives us the most relevant and generalized low-code related topics. \bf{External Validity} threats relate to the generalizability of our findings. Our study is based on data from developers' discussion on SO. However, there are other forums  LCSD developers may use to discuss. Nevertheless, we believe using SO's data provides us with generalizability because SO is a widely used Q\&A platform for developers. To ensure good quality discussion, we only use posts with non-negative scores. However, we also believe this study can be complemented by including discussions from other forums, surveying and interviewing low-code developers.

